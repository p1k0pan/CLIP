{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbddcfa-6da8-40f3-9f3f-a6f8bee43015",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ltstorage/home/2pan/dataset/VG_Attribution\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m annotation_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual_genome_attribution.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnare_datasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VG_Attribution\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clip\n",
      "File \u001b[0;32m~/CLIP/datasets_zoo/snare_datasets.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01measydict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EasyDict \u001b[38;5;28;01mas\u001b[39;00m edict\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_url\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_des\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Text_Des\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pre_caption\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "root_dir=\"/ltstorage/home/2pan/dataset/VG_Attribution\"\n",
    "annotation_file = os.path.join(root_dir, \"visual_genome_attribution.json\")\n",
    "from datasets_zoo import snare_datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from clip import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26671da4-c06e-4cfd-ae04-87ab38ce34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "test_dataset = snare_datasets.VG_Attribution(preprocess, attribute_ownership=True, dataset=dataset)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size = BATCH_SIZE, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ff76c-52a3-471d-9bca-f77c6d0b8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, device, validate=False):\n",
    "    model.eval()\n",
    "\n",
    "    print('Computing features for evaluation...')\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('eval_loss', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Evaluation loss'\n",
    "    print_freq = 50\n",
    "    step=0\n",
    "    total_loss = 0\n",
    "    scores = []\n",
    "    for i,batch in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "    # for batch in train_dataloader :\n",
    "        step+=1\n",
    "        \n",
    "        image_options = []\n",
    "        image_embeddings = model.encode_image(batch[\"image_options\"].to(device)).cpu()  # B x D\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=1, keepdim = True)\n",
    "        image_options.append(np.expand_dims(image_embeddings.numpy(), axis=1))\n",
    "\n",
    "        caption_options = []\n",
    "        # print(batch['caption_options'])\n",
    "        \"\"\"\n",
    "        [('a road with a red dirt on a small moped on a man helmet',), \n",
    "        ('a man a red helmet a a small moped on on dirt road with',), \n",
    "        ('man a a small on helmet a red with moped on dirt road a',)]\n",
    "\n",
    "        \"A man with a red helmet on a small moped on a dirt road. \", \n",
    "        \n",
    "        \"\"\"\n",
    "        cur_loss = 0\n",
    "        for idx in range(len(batch[\"caption_options\"])):\n",
    "            caption_tokenized = torch.cat([clip.tokenize(c) for c in batch[\"caption_options\"][idx]])\n",
    "            caption_embeddings = model.encode_text(caption_tokenized.to(device)).cpu()  # B x D\n",
    "\n",
    "            # caption_embeddings = caption_embeddings - self.blank\n",
    "\n",
    "            # caption_embeddings = caption_embeddings / np.linalg.norm(caption_embeddings, axis=1,\n",
    "            #                                                             keepdims=True)  # B x D\n",
    "            caption_embeddings = caption_embeddings / caption_embeddings.norm(dim=1, keepdim = True)\n",
    "            caption_options.append(np.expand_dims(caption_embeddings.numpy(), axis=1))\n",
    "            if validate:\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "                with autocast():\n",
    "                    logits_per_image = image_embeddings @ caption_embeddings.t()\n",
    "                    logits_per_text = logits_per_image.T\n",
    "                    # print(logits_per_image.size())\n",
    "                    if idx != 1:\n",
    "                        ground_truth = torch.eye(logits_per_image.size(0),device=\"cpu\")\n",
    "                        cur_loss += (loss_func(logits_per_image,ground_truth) + loss_func(logits_per_text,ground_truth))/2\n",
    "                    else:\n",
    "                        negative_ground_truth = torch.zeros_like(logits_per_image,device=\"cpu\")\n",
    "                        cur_loss += (loss_func(logits_per_image,negative_ground_truth) + loss_func(logits_per_text,negative_ground_truth))/2\n",
    "\n",
    "                    total_loss += cur_loss.item()\n",
    "                wandb.log({\"eval_loss\":cur_loss.item()})\n",
    "\n",
    "        # print(len(caption_options))\n",
    "        image_options = np.concatenate(image_options, axis=1)  # B x K x D\n",
    "        # print(\"image_options\", image_options.shape)\n",
    "        caption_options = np.concatenate(caption_options, axis=1)  # B x L x D\n",
    "        # print(\"caption_options\", caption_options.shape)\n",
    "        batch_scores = np.einsum(\"nkd,nld->nkl\", image_options, caption_options)  # B x K x L\n",
    "\n",
    "\n",
    "        scores.append(batch_scores)\n",
    "    if validate:\n",
    "        wandb.log({\"eval_avg_loss\":total_loss/step})\n",
    "    print(metric_logger.global_avg())\n",
    "    all_scores = np.concatenate(scores, axis=0)  # N x K x L\n",
    "    print(\"all_scores\", all_scores.shape)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70f71e-b94d-43c6-8491-fd103b1e9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "convert_models_to_fp32(model, True)\n",
    "# print(test_dataset.all_attributes)\n",
    "all_scores=evaluation(model, test_dataloader, \"cuda:1\", False)\n",
    "if test_dataset is not None:\n",
    "    test_result = test_dataset.evaluate_scores(all_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
